name: Monitor External Release

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

permissions:
  contents: read
  actions: write

env:
  TARGET_REPOSITORIES: ${{ vars.TARGET_REPOSITORIES }}
  TARGET_REPOSITORY: ${{ vars.TARGET_REPOSITORY }}
  MATTERMOST_WEBHOOK_URL: ${{ secrets.MATTERMOST_WEBHOOK_URL }}
  STATE_ARTIFACT_PREFIX: release-monitor-state
  STATE_FILE: state.json
  GEMINI_MODEL: ${{ vars.GEMINI_MODEL }}

jobs:
  prepare-targets:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.prepare.outputs.matrix }}
    steps:
      - name: リポジトリリスト生成
        id: prepare
        env:
          TARGET_REPOSITORIES: ${{ env.TARGET_REPOSITORIES }}
          TARGET_REPOSITORY: ${{ env.TARGET_REPOSITORY }}
          STATE_ARTIFACT_PREFIX: ${{ env.STATE_ARTIFACT_PREFIX }}
        run: |
          set -euo pipefail
          python <<'PY'
          import json
          import os
          import re
          import sys
          def normalize_list(raw):
              values = []
              if not raw:
                  return values
              for part in re.split(r'[\s,]+', raw):
                  part = part.strip()
                  if part:
                      values.append(part)
              return values

          targets = []
          seen = set()
          prefix = os.environ.get("STATE_ARTIFACT_PREFIX", "release-monitor-state")
          for source in (
              os.environ.get("TARGET_REPOSITORIES") or "",
              os.environ.get("TARGET_REPOSITORY") or "",
          ):
              for repo in normalize_list(source):
                  if repo in seen:
                      continue
                  seen.add(repo)
                  slug = re.sub(r'[^0-9A-Za-z_.-]+', '-', repo).strip("-")
                  slug = slug.lower() or "default"
                  targets.append({
                      "repo": repo,
                      "artifact": f"{prefix}-{slug}",
                  })

          if not targets:
              print("TARGET_REPOSITORIES または TARGET_REPOSITORY が未設定です", file=sys.stderr)
              sys.exit(1)

          matrix = json.dumps(targets)
          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as fh:
              fh.write(f"matrix={matrix}\n")
          print(f"Monitoring {len(targets)} repositories: {[t['repo'] for t in targets]}")
          PY

  watch-release:
    needs: prepare-targets
    runs-on: ubuntu-latest
    strategy:
      matrix:
        target: ${{ fromJSON(needs.prepare-targets.outputs.matrix) }}
    env:
      TARGET_REPO: ${{ matrix.target.repo }}
      STATE_ARTIFACT_NAME: ${{ matrix.target.artifact }}
    steps:
      - name: 設定検証
        run: |
          set -euo pipefail
          if [ -z "$TARGET_REPO" ]; then
            echo "リポジトリが未設定です" >&2
            exit 1
          fi
          if [ -z "$MATTERMOST_WEBHOOK_URL" ]; then
            echo "Secrets.MATTERMOST_WEBHOOK_URL が未設定です" >&2
            exit 1
          fi

      - name: 最新リリース取得
        id: fetch
        env:
          TARGET_REPO: ${{ env.TARGET_REPO }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          python <<'PY'
          import base64
          import json
          import os
          import sys
          import urllib.error
          import urllib.request

          repo = os.environ.get('TARGET_REPO')
          if not repo:
              print('TARGET_REPO is required', file=sys.stderr)
              sys.exit(1)
          token = os.environ.get('GITHUB_TOKEN', '')

          base_headers = {
              'Accept': 'application/vnd.github+json',
              'User-Agent': 'release-monitor-action',
          }
          if token:
              base_headers['Authorization'] = f'Bearer {token}'

          def request(url, accept=None):
              headers = dict(base_headers)
              if accept:
                  headers['Accept'] = accept
              req = urllib.request.Request(url, headers=headers)
              try:
                  with urllib.request.urlopen(req, timeout=30) as resp:
                      return resp.getcode(), resp.read()
              except urllib.error.HTTPError as exc:
                  return exc.code, exc.read()
              except Exception as exc:
                  print(f'HTTP request failed: {exc}', file=sys.stderr)
                  return None, b''

          status, data = request(f'https://api.github.com/repos/{repo}')
          if status != 200 or not data:
              print(f'GitHub API error fetching repository info: HTTP {status}', file=sys.stderr)
              sys.exit(1)
          try:
              repo_info = json.loads(data.decode('utf-8'))
          except Exception as exc:
              print(f'Failed to parse repository info: {exc}', file=sys.stderr)
              sys.exit(1)

          default_branch = (repo_info.get('default_branch') or '').strip()
          entries = []

          def add_release_entry(release):
              tag = (release.get('tag_name') or '').strip()
              if not tag:
                  return
              name = (release.get('name') or tag).strip()
              target_commitish = (release.get('target_commitish') or '').strip()
              entries.append({
                  'tag': tag,
                  'name': name,
                  'url': release.get('html_url') or f'https://github.com/{repo}/releases/tag/{tag}',
                  'body': release.get('body') or '',
                  'reference_kind': 'release',
                  'reference_sha': target_commitish,
                  'default_branch': target_commitish or default_branch,
                  'fallback_branch': '',
                  'fallback_path': '',
                  'published_at': release.get('published_at') or '',
              })

          def add_tag_entry(tag_entry):
              tag_name = (tag_entry.get('name') or '').strip()
              if not tag_name:
                  return
              commit_sha = ''
              commit = tag_entry.get('commit') or {}
              if isinstance(commit, dict):
                  commit_sha = (commit.get('sha') or '').strip()
              entries.append({
                  'tag': tag_name,
                  'name': tag_name,
                  'url': f'https://github.com/{repo}/tree/{tag_name}',
                  'body': '',
                  'reference_kind': 'tag',
                  'reference_sha': commit_sha,
                  'default_branch': default_branch,
                  'fallback_branch': default_branch,
                  'fallback_path': '',
                  'published_at': '',
              })

          def add_changelog_entry(path, commit):
              sha = (commit.get('sha') or '').strip()
              if not sha:
                  return
              tag = f'changelog-{sha[:12]}'
              entries.append({
                  'tag': tag,
                  'name': f'{path} update',
                  'url': f'https://github.com/{repo}/blob/{default_branch}/{path}',
                  'body': '',
                  'reference_kind': 'changelog',
                  'reference_sha': sha,
                  'default_branch': default_branch,
                  'fallback_branch': default_branch,
                  'fallback_path': path,
                  'published_at': commit.get('commit', {}).get('author', {}).get('date', ''),
              })

          status, data = request(f'https://api.github.com/repos/{repo}/releases?per_page=20')
          if status == 200 and data:
              try:
                  releases = json.loads(data.decode('utf-8', errors='replace'))
              except Exception as exc:
                  print(f'Failed to parse releases list: {exc}', file=sys.stderr)
                  releases = []
              if isinstance(releases, list):
                  for release in releases:
                      add_release_entry(release)

          if not entries:
              status, data = request(f'https://api.github.com/repos/{repo}/tags?per_page=20')
              if status == 200 and data:
                  try:
                      tags = json.loads(data.decode('utf-8'))
                  except Exception as exc:
                      print(f'Failed to parse tags: {exc}', file=sys.stderr)
                      tags = []
                  if isinstance(tags, list):
                      for tag_entry in tags:
                          add_tag_entry(tag_entry)

          if not entries and default_branch:
              paths = [
                  'CHANGELOG.md',
                  'CHANGE.md',
                  'docs/CHANGELOG.md',
                  'docs/CHANGE.md',
                  'CHANGELOG',
                  'docs/CHANGELOG',
              ]
              for path in paths:
                  status, data = request(f'https://api.github.com/repos/{repo}/commits?sha={default_branch}&path={path}&per_page=5')
                  if status != 200 or not data:
                      continue
                  try:
                      commits = json.loads(data.decode('utf-8'))
                  except Exception as exc:
                      print(f'Failed to parse commits for {path}: {exc}', file=sys.stderr)
                      continue
                  if not isinstance(commits, list):
                      continue
                  for commit in commits:
                      add_changelog_entry(path, commit)
                  if entries:
                      break

          outputs = {}
          if entries:
              first = entries[0]
              outputs = {
                  'release_found': 'true',
                  'reference_kind': first.get('reference_kind', ''),
                  'tag': first.get('tag', ''),
                  'name': first.get('name', ''),
                  'url': first.get('url', ''),
                  'body': first.get('body', ''),
                  'default_branch': first.get('default_branch', ''),
                  'fallback_branch': first.get('fallback_branch', ''),
                  'fallback_path': first.get('fallback_path', ''),
                  'reference_sha': first.get('reference_sha', ''),
              }
          else:
              outputs = {
                  'release_found': 'false',
                  'reference_kind': '',
                  'tag': '',
                  'name': '',
                  'url': '',
                  'body': '',
                  'default_branch': default_branch,
                  'fallback_branch': '',
                  'fallback_path': '',
                  'reference_sha': '',
              }

          entries_json = json.dumps(entries, ensure_ascii=False)
          entries_b64 = base64.b64encode(entries_json.encode('utf-8')).decode('ascii')

          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              for key, value in outputs.items():
                  value = value or ''
                  if key == 'body':
                      if value:
                          fh.write('body<<EOF\n')
                          fh.write(value)
                          fh.write('\nEOF\n')
                      else:
                          fh.write('body=\n')
                      continue
                  fh.write(f'{key}={value}\n')
              fh.write(f'entries_b64={entries_b64}\n')
          PY

      - name: 状態読み込み
        if: steps.fetch.outputs.release_found == 'true'
        id: state
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARTIFACT_NAME: ${{ env.STATE_ARTIFACT_NAME }}
          STATE_FILE: ${{ env.STATE_FILE }}
        run: |
          set -euo pipefail
          response=$(curl -sS \
            -H 'Accept: application/vnd.github+json' \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            "https://api.github.com/repos/${GITHUB_REPOSITORY}/actions/artifacts?per_page=100")

          download_url=$(printf '%s\n' "$response" | jq -r --arg name "$ARTIFACT_NAME" \
            '[.artifacts[] | select(.name == $name and .expired == false)] | sort_by(.created_at) | last | .archive_download_url')


          artifact_id=$(printf '%s\n' "$response" | jq -r --arg name "$ARTIFACT_NAME" \
            '[.artifacts[] | select(.name == $name and .expired == false)] | sort_by(.created_at) | last | .id')

          if [ -z "$download_url" ] || [ "$download_url" = "null" ]; then
            echo "exists=false" >> "$GITHUB_OUTPUT"
            echo "last_tag=" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          curl -sS -L \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -o artifact.zip \
            "$download_url"

          unzip -p artifact.zip "$STATE_FILE" > "$STATE_FILE" || true
          rm -f artifact.zip

          last_tag=$(jq -r '.last_tag // ""' "$STATE_FILE" 2>/dev/null || echo "")

          echo "exists=true" >> "$GITHUB_OUTPUT"
          printf 'artifact_id=%s\n' "$artifact_id" >> "$GITHUB_OUTPUT"
          printf 'last_tag=%s\n' "$last_tag" >> "$GITHUB_OUTPUT"

          if [ -n "$artifact_id" ] && [ "$artifact_id" != "null" ]; then
            curl -sS -X DELETE \
              -H 'Accept: application/vnd.github+json' \
              -H "Authorization: Bearer $GITHUB_TOKEN" \
              "https://api.github.com/repos/${GITHUB_REPOSITORY}/actions/artifacts/${artifact_id}" >/dev/null
          fi

      - name: 新規エントリ抽出
        if: steps.fetch.outputs.release_found == 'true'
        id: filter
        env:
          ENTRIES_B64: ${{ steps.fetch.outputs.entries_b64 }}
          LAST_TAG: ${{ steps.state.outputs.last_tag }}
        run: |
          set -euo pipefail
          python <<'PY'
          import base64
          import json
          import os

          entries_b64 = os.environ.get('ENTRIES_B64', '') or ''
          if entries_b64:
              try:
                  entries = json.loads(base64.b64decode(entries_b64).decode('utf-8'))
              except Exception:
                  entries = []
          else:
              entries = []

          last_tag = (os.environ.get('LAST_TAG') or '').strip()
          new_entries = []
          found_last = False
          for entry in entries:
              tag = (entry.get('tag') or '').strip()
              if last_tag and tag == last_tag:
                  found_last = True
                  break
              new_entries.append(entry)

          if last_tag and not found_last and entries and new_entries and len(new_entries) == len(entries):
              pass

          new_entries.reverse()
          filtered_json = json.dumps(new_entries, ensure_ascii=False)
          filtered_b64 = base64.b64encode(filtered_json.encode('utf-8')).decode('ascii')

          latest_tag = ''
          if entries:
              latest_tag = (entries[0].get('tag') or '').strip()

          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              fh.write(f'entry_count={len(new_entries)}\n')
              fh.write(f'entries_b64={filtered_b64}\n')
              fh.write(f'latest_tag={latest_tag}\n')
          PY

      - name: 新規リリース判定
        if: steps.fetch.outputs.release_found == 'true'
        id: compare
        env:
          ENTRY_COUNT: ${{ steps.filter.outputs.entry_count }}
        run: |
          set -euo pipefail
          python <<'PY'
          import os

          try:
              entry_count = int(os.environ.get('ENTRY_COUNT', '0') or '0')
          except ValueError:
              entry_count = 0

          changed = 'true' if entry_count > 0 else 'false'

          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              fh.write(f'changed={changed}\n')
          PY

      - name: リリースノート収集
        if: steps.compare.outputs.changed == 'true'
        id: content
        env:
          TARGET_REPO: ${{ env.TARGET_REPO }}
          ENTRIES_B64: ${{ steps.filter.outputs.entries_b64 }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          python <<'PY'
          import base64
          import json
          import os
          import re
          import subprocess
          import sys
          import urllib.error
          import urllib.request

          def log(msg):
              print(msg, file=sys.stderr)

          entries_b64 = os.environ.get('ENTRIES_B64', '') or ''
          try:
              entries = json.loads(base64.b64decode(entries_b64).decode('utf-8'))
          except Exception:
              entries = []

          target_repo = os.environ.get('TARGET_REPO', '')
          token = os.environ.get('GITHUB_TOKEN', '')

          def build_candidates(tag_value):
              values = []
              if not tag_value:
                  return values
              values.append(tag_value)
              tail = tag_value.split('/')[-1]
              values.append(tail)
              if tail.startswith('v'):
                  values.append(tail[1:])
              if tag_value.startswith('v'):
                  values.append(tag_value[1:])
              match = re.search(r'(\d+\.\d+(?:\.\d+)*)', tag_value)
              if match:
                  values.append(match.group(1))
              normed = []
              seen = set()
              for item in values:
                  normalized = item.lower()
                  if normalized and normalized not in seen:
                      normed.append(item)
                      seen.add(normalized)
              return normed

          def extract_section(text, candidates):
              if not text.strip():
                  return ''
              lines = text.splitlines()
              fallback_idx = None
              for idx, line in enumerate(lines):
                  m = re.match(r'^\s*(#{1,6})\s+(.*)$', line)
                  if not m:
                      continue
                  level = len(m.group(1))
                  heading_text = re.sub(r'[\[\]()]', '', m.group(2))
                  heading_norm = re.sub(r'\s+', ' ', heading_text).lower()
                  if any(c.lower() in heading_norm for c in candidates):
                      start = idx + 1
                      end = len(lines)
                      for j in range(start, len(lines)):
                          m2 = re.match(r'^\s*(#{1,6})\s+', lines[j])
                          if m2 and len(m2.group(1)) <= level:
                              end = j
                              break
                      section = '\n'.join(lines[start:end]).strip()
                      if section:
                          return section
                      fallback_idx = idx
              if fallback_idx is not None:
                  start = fallback_idx + 1
                  excerpt = '\n'.join(lines[start:start + 40]).strip()
                  if excerpt:
                      return excerpt
              trimmed = text.strip()
              if len(trimmed) > 4000:
                  return trimmed[:4000].rstrip() + '\n...'
              return trimmed

          def fetch_changelog(entry, candidates):
              reference_kind = (entry.get('reference_kind') or '').lower()
              default_branch = entry.get('default_branch') or ''
              fallback_branch = entry.get('fallback_branch') or ''
              fallback_path = entry.get('fallback_path') or ''
              reference_sha = entry.get('reference_sha') or ''

              base_paths = [
                  'CHANGELOG.md',
                  'CHANGE.md',
                  'docs/CHANGELOG.md',
                  'docs/CHANGE.md',
                  'CHANGELOG',
                  'docs/CHANGELOG',
              ]

              ordered_paths = []
              if fallback_path:
                  ordered_paths.append(fallback_path)
              for path in base_paths:
                  if path not in ordered_paths:
                      ordered_paths.append(path)

              ref_candidates = []
              if reference_kind in ('release', 'tag'):
                  tag = entry.get('tag') or ''
                  if tag:
                      ref_candidates.append(tag)
                  if reference_sha and reference_sha not in ref_candidates:
                      ref_candidates.append(reference_sha)
              elif reference_kind == 'changelog':
                  if reference_sha:
                      ref_candidates.append(reference_sha)
                  if fallback_branch and fallback_branch not in ref_candidates:
                      ref_candidates.append(fallback_branch)
              if default_branch and default_branch not in ref_candidates:
                  ref_candidates.append(default_branch)
              if not ref_candidates:
                  ref_candidates = [None]

              for path in ordered_paths:
                  for ref in ref_candidates:
                      url = f'https://api.github.com/repos/{target_repo}/contents/{path}'
                      if ref:
                          url = f'{url}?ref={ref}'
                      headers = {
                          'Accept': 'application/vnd.github.raw',
                          'User-Agent': 'release-monitor-action',
                      }
                      if token:
                          headers['Authorization'] = f'Bearer {token}'
                      req = urllib.request.Request(url, headers=headers)
                      try:
                          with urllib.request.urlopen(req) as resp:
                              data = resp.read().decode('utf-8', errors='replace')
                      except urllib.error.HTTPError as exc:
                          if exc.code not in (403, 404):
                              ref_label = ref or 'default'
                              log(f'GitHub API error for {path} (ref {ref_label}): {exc}')
                          if exc.code == 403 and not token:
                              log('GitHub API rate limited and no token provided.')
                          continue
                      except Exception as exc:
                          log(f'Failed to fetch {path}: {exc}')
                          continue
                      section = extract_section(data, candidates)
                      if section:
                          return section, f'changelog:{path}'
                      trimmed = data.strip()
                      if trimmed:
                          if len(trimmed) > 4000:
                              trimmed = trimmed[:4000].rstrip() + '\n...'
                          return trimmed, f'changelog:{path}'
              return None, None

          def fetch_npm(entry, candidates):
              package = os.environ.get('NPM_PACKAGE') or ''
              if not package:
                  if target_repo and '/' in target_repo:
                      package = target_repo.split('/')[-1]
              if not package:
                  return None, None
              tag = entry.get('tag') or ''
              version = ''
              if tag:
                  version = tag.lstrip('v')
                  match = re.search(r'(\d+\.\d+(?:\.\d+)*)', tag)
                  if match:
                      version = match.group(1)
              if not version:
                  return None, None
              try:
                  proc = subprocess.run(
                      ['npm', 'view', f'{package}@{version}', '--json'],
                      stdout=subprocess.PIPE,
                      stderr=subprocess.PIPE,
                      text=True,
                      check=True,
                  )
              except subprocess.CalledProcessError as exc:
                  log(f'npm view failed: {exc.stderr.strip() or exc.stdout.strip() or exc}')
                  return None, None
              raw = proc.stdout.strip()
              if not raw or raw in ('null', 'undefined'):
                  return None, None
              try:
                  data = json.loads(raw)
              except json.JSONDecodeError as exc:
                  log(f'Failed to parse npm output: {exc}')
                  return None, None
              if isinstance(data, list):
                  data = data[0] if data else {}
              parts = []
              description = data.get('description')
              if description:
                  parts.append(description.strip())
              release_notes = data.get('releaseNotes') or data.get('changelog')
              if release_notes:
                  parts.append(str(release_notes).strip())
              readme = data.get('readme')
              if readme:
                  section = extract_section(readme, candidates)
                  if section:
                      parts.append(section)
              time_info = data.get('time') or {}
              published_at = time_info.get(version)
              if published_at:
                  parts.append(f'Published on npm: {published_at}')
              homepage = data.get('homepage')
              if homepage:
                  parts.append(f'Homepage: {homepage}')
              if not parts:
                  return None, None
              text = '\n\n'.join(part for part in parts if part).strip()
              if len(text) > 4000:
                  text = text[:4000].rstrip() + '\n...'
              return text, f'npm:{package}@{version}'

          results = []
          for entry in entries:
              release_body = (entry.get('body') or '').strip()
              content = release_body
              source = 'release' if release_body else ''
              candidates = build_candidates(entry.get('tag') or '')

              if not content:
                  changelog_text, changelog_source = fetch_changelog(entry, candidates)
                  if changelog_text:
                      content = changelog_text.strip()
                      source = changelog_source or 'changelog'

              if not content:
                  npm_text, npm_source = fetch_npm(entry, candidates)
                  if npm_text:
                      content = npm_text.strip()
                      source = npm_source or 'npm'

              if not content:
                  source = source or 'none'

              results.append({
                  'tag': entry.get('tag', ''),
                  'name': entry.get('name', ''),
                  'url': entry.get('url', ''),
                  'content': content.strip(),
                  'source': source,
              })

          processed_json = json.dumps(results, ensure_ascii=False)
          processed_b64 = base64.b64encode(processed_json.encode('utf-8')).decode('ascii')

          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              fh.write(f'processed_b64={processed_b64}\n')
              fh.write(f'processed_count={len(results)}\n')
          PY

      - name: リリースノート翻訳
        if: steps.compare.outputs.changed == 'true'
        id: translate
        env:
          PROCESSED_B64: ${{ steps.content.outputs.processed_b64 }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GEMINI_MODEL: ${{ env.GEMINI_MODEL }}
        run: |
          set -euo pipefail
          if [ -z "$GEMINI_API_KEY" ]; then
            echo "Secrets.GEMINI_API_KEY が未設定です" >&2
            exit 1
          fi
          python3 -m pip install --upgrade --quiet google-generativeai
          python <<'PY'
          import base64
          import json
          import os
          import sys
          import textwrap

          processed_b64 = os.environ.get('PROCESSED_B64', '') or ''
          try:
              entries = json.loads(base64.b64decode(processed_b64).decode('utf-8'))
          except Exception:
              entries = []

          api_key = os.environ.get('GEMINI_API_KEY')
          if not api_key:
              print('Gemini API key is missing', file=sys.stderr)
              sys.exit(1)

          model_name = os.environ.get('GEMINI_MODEL') or 'gemini-1.5-flash'

          try:
              import google.generativeai as genai
          except ImportError as exc:
              print(f'google.generativeai import failed: {exc}', file=sys.stderr)
              sys.exit(1)

          genai.configure(api_key=api_key)
          model = genai.GenerativeModel(model_name)

          for entry in entries:
              body = (entry.get('content') or '').strip()
              if not body:
                  entry['translated'] = ''
                  continue
              prompt = textwrap.dedent(
                  """
                  あなたはリリースノートを日本語に翻訳するアシスタントです。次のテンプレートに従い、プレーンテキストで出力してください（コードフェンス禁止）。

                  リリースノート

                  新機能
                  <新機能があれば箇条書きで記載。なければ「該当なし」と書く。>

                  マージされたPR:
                  <関連するPR番号と要約を箇条書きで記載。情報が無ければ「情報なし」と書く。>

                  主な更新点:
                  <3〜5行程度で日本語の要約を記載。必要なら箇条書きを使用。>

                  各セクションでは角括弧付きの指示文を出力せず、実際の内容もしくは指定された定型文だけを記載してください。

                  入力テキスト:
                  -----
                  {body}
                  """
              ).format(body=body)
              try:
                  response = model.generate_content(prompt)
              except Exception as exc:
                  print(f'Gemini generate_content failed for {entry.get("tag")}: {exc}', file=sys.stderr)
                  entry['translated'] = ''
                  continue
              text = ''
              if hasattr(response, 'text') and response.text:
                  text = response.text
              else:
                  for part in getattr(response, 'parts', []) or []:
                      text_part = getattr(part, 'text', None)
                      if text_part:
                          text += text_part
              entry['translated'] = text.strip()

          translated_json = json.dumps(entries, ensure_ascii=False)
          translated_b64 = base64.b64encode(translated_json.encode('utf-8')).decode('ascii')

          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              fh.write(f'translated_b64={translated_b64}\n')
          PY

      - name: Mattermostメッセージ生成
        if: steps.compare.outputs.changed == 'true'
        id: message
        env:
          TARGET_REPO: ${{ env.TARGET_REPO }}
          TRANSLATED_B64: ${{ steps.translate.outputs.translated_b64 }}
        run: |
          python <<'PY'
          import base64
          import json
          import os

          def truncate(text, limit=1800):
              text = text.strip()
              if len(text) <= limit:
                  return text
              return text[:limit].rstrip() + '\n...'

          translated_b64 = os.environ.get('TRANSLATED_B64', '') or ''
          try:
              entries = json.loads(base64.b64decode(translated_b64).decode('utf-8'))
          except Exception:
              entries = []

          target_repo = os.environ.get('TARGET_REPO', '')

          payloads = []
          for entry in entries:
              tag = entry.get('tag', 'unknown')
              release_name = entry.get('name') or tag
              url = entry.get('url', '')
              translated = (entry.get('translated') or '').strip()
              content = entry.get('content') or ''
              source_key = entry.get('source') or ''

              lines = [
                  f"Release `{tag}` detected for *{target_repo}*",
                  f"Title: {release_name}",
                  f"URL: {url}",
              ]

              def describe_source(key):
                  if not key or key == 'none':
                      return None
                  if key == 'release':
                      return '情報ソース: GitHub Release'
                  if key.startswith('changelog:'):
                      return f"情報ソース: {key.split(':', 1)[1]}"
                  if key.startswith('npm:'):
                      return f"情報ソース: npm {key.split(':', 1)[1]}"
                  return f"情報ソース: {key}"

              source_line = describe_source(source_key)
              if source_line:
                  lines.append(source_line)

              if translated:
                  lines.append('')
                  lines.extend(translated.splitlines())
              else:
                  fallback = truncate(content)
                  if fallback:
                      lines.append('')
                      lines.append('リリースノート')
                      lines.append('')
                      lines.append(fallback)
                  else:
                      lines.append('')
                      lines.append('リリースノート情報を取得できませんでした。')

              payloads.append(json.dumps({'text': '\n'.join(lines)}, ensure_ascii=False))

          payloads_json = json.dumps(payloads, ensure_ascii=False)
          payloads_b64 = base64.b64encode(payloads_json.encode('utf-8')).decode('ascii')

          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              fh.write(f'payloads_b64={payloads_b64}\n')
              fh.write(f'payload_count={len(payloads)}\n')
          PY

      - name: Mattermostへ送信
        if: steps.compare.outputs.changed == 'true'
        env:
          MATTERMOST_WEBHOOK_URL: ${{ env.MATTERMOST_WEBHOOK_URL }}
          PAYLOADS_B64: ${{ steps.message.outputs.payloads_b64 }}
        run: |
          set -euo pipefail
          python <<'PY'
          import base64
          import json
          import os
          import sys
          import urllib.error
          import urllib.request

          webhook = os.environ.get('MATTERMOST_WEBHOOK_URL')
          if not webhook:
              print('MATTERMOST_WEBHOOK_URL is missing', file=sys.stderr)
              sys.exit(1)

          payloads_b64 = os.environ.get('PAYLOADS_B64', '') or ''
          try:
              payloads = json.loads(base64.b64decode(payloads_b64).decode('utf-8'))
          except Exception:
              payloads = []

          if not payloads:
              print('No payload to send', file=sys.stderr)
              sys.exit(0)

          for payload in payloads:
              data = payload.encode('utf-8')
              req = urllib.request.Request(
                  webhook,
                  data=data,
                  headers={'Content-Type': 'application/json'},
              )
              try:
                  with urllib.request.urlopen(req) as resp:
                      resp.read()
              except urllib.error.HTTPError as exc:
                  print(f'Mattermost webhook error: {exc.status} {exc.reason}', file=sys.stderr)
                  body = exc.read().decode('utf-8', errors='replace')
                  print(body, file=sys.stderr)
                  raise
          PY

      - name: 状態ファイル作成
        if: steps.fetch.outputs.release_found == 'true'
        env:
          STATE_FILE: ${{ env.STATE_FILE }}
          LAST_TAG: ${{ steps.state.outputs.last_tag }}
          LATEST_TAG: ${{ steps.filter.outputs.latest_tag }}
          CHANGED: ${{ steps.compare.outputs.changed }}
        run: |
          set -euo pipefail
          tag="$LAST_TAG"
          if [ "$CHANGED" = "true" ]; then
            tag="$LATEST_TAG"
          fi
          printf '{"last_tag":"%s"}\n' "$tag" > "$STATE_FILE"

      - name: 状態保存
        if: steps.fetch.outputs.release_found == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.STATE_ARTIFACT_NAME }}
          path: ${{ env.STATE_FILE }}
          overwrite: true
